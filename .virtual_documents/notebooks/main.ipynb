import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob # for combining csvs


data_files = glob.glob('../data/*.csv')





df = pd.concat(
  (pd.read_csv(filename) for filename in data_files), 
  ignore_index=True
)

df
df_raw = df





df.info()





df['tourney_date'] = pd.to_datetime(df.tourney_date, format = '%Y%m%d')


df = df.sort_values(by=['tourney_date', 'tourney_name', 'match_num'])
df.head(30)





df.shape


df.drop_duplicates(inplace=True)


df.shape # no duplicates


# checking for nulls
df.isna().sum(axis=0).sort_values(ascending=False)


missing_stats = df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]
missing_stats





df.drop(columns=['winner_entry', 'loser_entry', 'loser_seed', 'winner_seed'], inplace=True)


missing_stats = df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]
missing_stats





df.drop(columns=['minutes','w_ace','w_df','w_svpt','w_1stIn','w_1stWon','w_2ndWon','w_SvGms','w_bpSaved','w_bpFaced','l_ace','l_df','l_svpt','l_1stIn','l_1stWon','l_2ndWon','l_SvGms','l_bpSaved','l_bpFaced'], inplace=True)


missing_stats = df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]
missing_stats





df.dropna(subset=['loser_ht', 'winner_ht'], inplace=True)


df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]





df.drop(columns=['loser_rank_points', 'winner_rank_points'], inplace=True)


df.dropna(subset=['loser_rank', 'winner_rank'], inplace=True)


df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]








df


df.info()





tourney_fltr = ((df['tourney_level'] == 'M') | (df['tourney_level'] == 'G') | (df['tourney_level'] == 'A'))

df = df.loc[tourney_fltr, :]


df['tourney_level'].unique()


df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]





df['surface'].value_counts()











df = df.sort_values(by=['tourney_date', 'tourney_name', 'match_num'])
df.head(10)





# sample of the data first
df_sample = df.head(1000).copy()


# create new columns
df_sample['winner_surface_win_pct'] = None
df_sample['loser_surface_win_pct'] = None
df_sample['winner_surface_num_matches'] = None
df_sample['loser_surface_num_matches'] = None
df_sample


def safe_divide(a, b):
    if b == 0:
        return 0
    else:
        return a / b


# get all player names 
winner_names = df_sample['winner_name'].unique()
loser_names = df_sample['loser_name'].unique()



# remove duplicates
combine_names = np.hstack((winner_names, loser_names))
unique_names = set(combine_names)


# initialize the objects for building my featured columns
cumu_surface_num_matches = {}
cumu_sum_by_player = {}
for name in unique_names:
    cumu_surface_num_matches[name] = {'Hard': 0,
                                      'Clay': 0,
                                      'Grass': 0,
                                      'Carpet': 0}
    
    cumu_sum_by_player[name] = {'Hard': [0, 0],
                                  'Clay': [0, 0],
                                  'Grass': [0, 0],
                                  'Carpet': [0, 0]}


for index, row in df_sample.iterrows():
    winner_name = row['winner_name']
    loser_name = row['loser_name']
    surface = row['surface']

    # first get win_pct_on surface
    winner_total_wins = cumu_sum_by_player[winner_name][surface][0]
    winner_total_matches = sum(cumu_sum_by_player[winner_name][surface])
    winner_win_pct = safe_divide(winner_total_wins, winner_total_matches)

    # now get loser's win pct on surface
    loser_total_wins = cumu_sum_by_player[loser_name][surface][0]
    loser_total_matches = sum(cumu_sum_by_player[loser_name][surface])
    loser_win_pct = safe_divide(loser_total_wins, loser_total_matches)

    # now add both to respective columns in df
    df_sample.loc[index, 'winner_surface_win_pct'] = winner_win_pct
    df_sample.loc[index, 'loser_surface_win_pct'] = loser_win_pct

    # update winner's record
    cumu_sum_by_player[winner_name][surface][0] += 1

    # update loser's record
    cumu_sum_by_player[loser_name][surface][1] += 1

    
    # add number matches to respective columns
    df_sample.loc[index, 'winner_surface_num_matches'] = cumu_surface_num_matches[winner_name][surface]
    df_sample.loc[index, 'loser_surface_num_matches'] = cumu_surface_num_matches[loser_name][surface]

    # update winner and loser num matches
    cumu_surface_num_matches[winner_name][surface]+=1
    cumu_surface_num_matches[loser_name][surface]+=1





print(cumu_surface_num_matches['Brad Gilbert'])


print(cumu_sum_by_player['Brad Gilbert'])


df_sample[df['winner_name'] == 'Brad Gilbert']


df_sample[df['loser_name'] == 'Brad Gilbert']





# first add all additional columns
df['winner_surface_win_pct'] = None
df['loser_surface_win_pct'] = None
df['winner_surface_num_matches'] = None
df['loser_surface_num_matches'] = None
df


# get all player names 
winner_names = df['winner_name'].unique()
loser_names = df['loser_name'].unique()


# remove duplicates
combine_names = np.hstack((winner_names, loser_names))
unique_names = set(combine_names)


# initialize the objects for building my featured columns
cumu_surface_num_matches = {}
cumu_sum_by_player = {}
for name in unique_names:
    cumu_surface_num_matches[name] = {'Hard': 0,
                                      'Clay': 0,
                                      'Grass': 0,
                                      'Carpet': 0}
    
    cumu_sum_by_player[name] = {'Hard': [0, 0],
                                  'Clay': [0, 0],
                                  'Grass': [0, 0],
                                  'Carpet': [0, 0]}


for index, row in df.iterrows():
    winner_name = row['winner_name']
    loser_name = row['loser_name']
    surface = row['surface']

    # first get win_pct_on surface
    winner_total_wins = cumu_sum_by_player[winner_name][surface][0]
    winner_total_matches = sum(cumu_sum_by_player[winner_name][surface])
    winner_win_pct = safe_divide(winner_total_wins, winner_total_matches)

    # now get loser's win pct on surface
    loser_total_wins = cumu_sum_by_player[loser_name][surface][0]
    loser_total_matches = sum(cumu_sum_by_player[loser_name][surface])
    loser_win_pct = safe_divide(loser_total_wins, loser_total_matches)

    # now add both to respective columns in df
    df.loc[index, 'winner_surface_win_pct'] = winner_win_pct
    df.loc[index, 'loser_surface_win_pct'] = loser_win_pct

    # update winner's record
    cumu_sum_by_player[winner_name][surface][0] += 1

    # update loser's record
    cumu_sum_by_player[loser_name][surface][1] += 1

    
    # add number matches to respective columns
    df.loc[index, 'winner_surface_num_matches'] = cumu_surface_num_matches[winner_name][surface]
    df.loc[index, 'loser_surface_num_matches'] = cumu_surface_num_matches[loser_name][surface]

    # update winner and loser num matches
    cumu_surface_num_matches[winner_name][surface]+=1
    cumu_surface_num_matches[loser_name][surface]+=1


df


print(cumu_sum_by_player['Andreas Seppi'])








date_fltr = (df['tourney_date'] > '2016-01-01') & (df['tourney_date'] < '2016-12-31')
sample_df = df.loc[date_fltr, :]
pd.set_option('display.max_columns', None)
sample_df.sort_values(by=['tourney_date', 'tourney_id', 'match_num'])
































date_fltr = df['tourney_date'] > '2000-01-01'	

df = df.loc[date_fltr, :]


num_matches_fltr = (df['winner_surface_num_matches'] >= 3) & (df['loser_surface_num_matches'] >= 3)

df = df.loc[num_matches_fltr, :]


(df['winner_surface_num_matches'] < 3).sum()


df.info()





df.drop(columns=['round', 'surface', 'tourney_id', 'tourney_name', 'draw_size', 'tourney_date', 'match_num'], inplace=True)


df.isna().sum(axis=0).sort_values(ascending=False)/df.shape[0]





df['tourney_level'].value_counts()


tourney_fltr = ((df['tourney_level'] == 'M') | (df['tourney_level'] == 'G'))

df_filtered = df.loc[tourney_fltr, :].copy()


df_filtered['tourney_level'].value_counts()


df_filtered.info()





df_filtered['winner_surface_win_pct'] = pd.to_numeric(df['winner_surface_win_pct'])
df_filtered['loser_surface_win_pct'] = pd.to_numeric(df['loser_surface_win_pct'])
df_filtered['winner_surface_num_matches'] = pd.to_numeric(df['winner_surface_num_matches'])
df_filtered['loser_surface_num_matches'] = pd.to_numeric(df['loser_surface_num_matches'])


df_filtered.info()


df_filtered.sample(10)





df_filtered = df_filtered.select_dtypes('number')


df_filtered.info()





df_filtered.drop(columns=['winner_id', 'loser_id'], inplace=True)


df_filtered.info()


df_filtered





suffix_to_append = ['_rank', '_ht', '_age', '_surface_win_pct', '_surface_num_matches']
final_df = pd.DataFrame(columns=list("HR" + ele for ele in suffix_to_append) + 
                            list("LR" + ele for ele in suffix_to_append)  + ['best_of'] + ["HR_win"],
                      index=df_filtered.index)


final_df


for index,row in df_filtered.iterrows():
    if row["winner_rank"] < row["loser_rank"]:

        final_df.loc[index,list("HR" + ele for ele in suffix_to_append)] = list(df.loc[index,list("winner" + ele for ele in suffix_to_append)])
        final_df.loc[index,list("LR" + ele for ele in suffix_to_append)] = list(df.loc[index,list("loser" + ele for ele in suffix_to_append)])
        final_df.loc[index,"HR_win"] = 1
        final_df.loc[index,"best_of"] = row['best_of']
    else:

        final_df.loc[index,list("LR" + ele for ele in suffix_to_append)] = list(df.loc[index,list("winner" + ele for ele in suffix_to_append)])
        final_df.loc[index,list("HR" + ele for ele in suffix_to_append)] = list(df.loc[index,list("loser" + ele for ele in suffix_to_append)])
        final_df.loc[index,"HR_win"] = 0
        final_df.loc[index,"best_of"] = row['best_of']


final_df


final_df.info()


df_obj_cols = final_df.select_dtypes(include='object').columns

final_df[df_obj_cols] = final_df[df_obj_cols].apply(pd.to_numeric, errors='coerce')


final_df.info()





final_df['ranking_difference'] = final_df['LR_rank'] - final_df['HR_rank']











import matplotlib.pyplot as plt
import seaborn as sns


final_df['HR_win'].sum() / final_df.shape[0]





eda_df = final_df.copy()
eda_df.head(10)


# take the differences
eda_df['height_diff'] = eda_df['HR_ht'] - eda_df['LR_ht']
eda_df['age_diff'] = eda_df['HR_age'] - eda_df['LR_age']
eda_df['win_pct_diff'] = eda_df['HR_surface_win_pct'] - eda_df['LR_surface_win_pct']
eda_df['num_matches_diff'] = final_df['HR_surface_num_matches'] - final_df['LR_surface_num_matches']


eda_df





sns.boxplot(x='HR_win', y='ranking_difference', data=eda_df)
plt.xlabel('Target')
plt.ylabel('Ranking Difference')
plt.title('Ranking Difference vs Target')
plt.yscale('log')
plt.show()





sns.boxplot(x='HR_win', y='win_pct_diff', data=eda_df)
plt.xlabel('Target')
plt.ylabel('Win Pct Difference')
plt.title('Win Percentage Difference vs Target')
plt.show()





sns.boxplot(x='HR_win', y='height_diff', data=eda_df)
plt.xlabel('Target')
plt.ylabel('Height Difference')
plt.title('Height Difference vs Target')
plt.show()





sns.boxplot(x='HR_win', y='age_diff', data=eda_df)
plt.xlabel('Target')
plt.ylabel('Height Difference')
plt.title('Height Difference vs Target')
plt.show()





sns.boxplot(x='HR_win', y='num_matches_diff', data=eda_df)
plt.xlabel('Target')
plt.ylabel('Num Matches Difference')
plt.title('Num Matches Difference vs Target')
plt.show()





# best of 3 sets mean first
best_3_filtr = eda_df['best_of'] == 3

best_3_df = eda_df[best_3_filtr].copy()

best_3_df['HR_win'].sum() / best_3_df.shape[0]


# best of 5 sets mean now
best_5_filtr = eda_df['best_of'] == 5

best_5_df = eda_df[best_5_filtr].copy()

best_5_df['HR_win'].sum() / best_5_df.shape[0]














from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split


y = final_df['HR_win']
X = final_df.drop(columns=['HR_win'])

X


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)


# Fitting logistic regression model
logreg = LogisticRegression(max_iter=10000)
logreg.fit(X_train, y_train)

# Training and test score
print(f"Train score: {logreg.score(X_train, y_train)}")
print(f"Test score: {logreg.score(X_test, y_test)}")


coefficients = logreg.coef_[0]
coefficients


feature_names = X_train.columns

coefficients = logreg.coef_[0]

for feat, coef in zip(feature_names, coefficients):
    print(f"feature: {feat}, coefficent: {coef}")





from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg_scaled = LogisticRegression(C=1)
logreg_scaled.fit(X_train_scaled, y_train)

coefficients = logreg_scaled.coef_[0]


print(f"Train score: {logreg_scaled.score(X_train_scaled, y_train)}")
print(f"Test score: {logreg_scaled.score(X_test_scaled, y_test)}")


feature_names = X_train.columns

coefficients = logreg_scaled.coef_[0]

for feat, coef in zip(feature_names, coefficients):
    print(f"feature: {feat}, coefficent: {coef}")





plt.figure(figsize=(15, 15))
sns.heatmap(X.corr().round(1), cmap="coolwarm", annot=True)





final_df


final_df['height_diff'] = final_df['HR_ht'] - final_df['LR_ht']
final_df['age_diff'] = final_df['HR_age'] - final_df['LR_age']
final_df['win_pct_diff'] = final_df['HR_surface_win_pct'] - final_df['LR_surface_win_pct']
final_df['num_matches_diff'] = final_df['HR_surface_num_matches'] - final_df['LR_surface_num_matches']


final_df.drop(columns=['HR_ht', 'LR_ht', 'HR_age', 'LR_age', 'HR_rank', 'LR_rank', 'HR_surface_win_pct', \
                      'LR_surface_win_pct', 'HR_surface_num_matches', 'LR_surface_num_matches'], inplace=True)


final_df.sample(10)





y = final_df['HR_win']
X = final_df.drop(columns=['HR_win'])

X


plt.figure(figsize=(15, 15))
sns.heatmap(X.corr().round(1), cmap="coolwarm", annot=True)





from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming X and y are your features and labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit the Ridge logistic regression model (L2 regularization)
model_ridge = LogisticRegression(penalty='l2', C=10, solver='liblinear', max_iter=1000)  # C is the inverse of regularization strength
model_ridge.fit(X_train_scaled, y_train)



print(f"Train score: {model_ridge.score(X_train_scaled, y_train)}")
print(f"Test score: {model_ridge.score(X_test_scaled, y_test)}")


# Get the feature importance (coefficients)
feature_importance_ridge = model_ridge.coef_[0]

# Create a DataFrame to display feature importance
import pandas as pd
feature_names = X.columns
importance_df_ridge = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance_ridge
})

# Sort by absolute value of importance
importance_df_ridge['Abs_Importance'] = importance_df_ridge['Importance'].abs()
importance_df_ridge = importance_df_ridge.sort_values(by='Abs_Importance', ascending=False)

print(importance_df_ridge[['Feature', 'Importance']])








# make all the feature differences, and throw out the HR and LR, and then try random forest - nonlinear model
# add a couple more features (ie win pct on surface)


# obj = {player_name: [.87, .78, .85] 








final_df


final_df['height_diff'] = final_df['HR_ht'] - final_df['LR_ht']
final_df['age_diff'] = final_df['HR_age'] - final_df['LR_age']
final_df['win_pct_diff'] = final_df['HR_surface_win_pct'] - final_df['LR_surface_win_pct']
final_df['num_matches_diff'] = final_df['HR_surface_num_matches'] - final_df['LR_surface_num_matches']


final_df.drop(columns=['HR_ht', 'LR_ht', 'HR_age', 'LR_age', 'HR_rank', 'LR_rank', 'HR_surface_win_pct', \
                      'LR_surface_win_pct'], inplace=True)


final_df


final_df.drop(columns=['HR_surface_num_matches', 'LR_surface_num_matches'], inplace=True)


final_df


from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score


y = final_df['HR_win']
X = final_df.drop(columns=['HR_win'])

X


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)


# Without scaling
rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
rf.fit(X_train, y_train)


rf.score(X_train,y_train) # for unlimited depth trees, training acc always 1.0


rf.score(X_test,y_test)


# With scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

rf_scaled = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
rf_scaled.fit(X_train_scaled, y_train)


rf_scaled.score(X_train_scaled,y_train) # for unlimited depth trees, training acc always 1.0


rf_scaled.score(X_test_scaled,y_test)





# Logistic with difference of features
# Fitting logistic regression model
logreg = LogisticRegression(max_iter=10000)
logreg.fit(X_train, y_train)

# Training and test score
print(f"Train score: {logreg.score(X_train, y_train)}")
print(f"Test score: {logreg.score(X_test, y_test)}")








from sklearn.ensemble import GradientBoostingClassifier


y = final_df['HR_win']
X = final_df.drop(columns=['HR_win'])

X


# train test split, and scale the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


gbc = GradientBoostingClassifier()
gbc.fit(X_train_scaled, y_train)


print(f"Train score: {gbc.score(X_train_scaled, y_train)}")
print(f"Test score: {gbc.score(X_test_scaled, y_test)}")


# Get the feature importance
feature_importance = gbc.feature_importances_

# Create a DataFrame to display feature importance
feature_names = X.columns
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
})

# Sort by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

sns.barplot(data = importance_df, x = 'feature', y = 'importance')

plt.xticks(rotation = 90)
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.title('Feature importances (gradient boosting classifier)', size = 16)





















